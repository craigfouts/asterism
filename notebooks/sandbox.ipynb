{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(act='relu', **kwargs):\n",
    "    if act == 'relu':\n",
    "        return nn.ReLU(**kwargs)\n",
    "    elif act == 'softplus':\n",
    "        return nn.Softplus(**kwargs)\n",
    "    elif act == 'sigmoid':\n",
    "        return nn.Sigmoid(**kwargs)\n",
    "    elif act == 'tanh':\n",
    "        return nn.Tanh(**kwargs)\n",
    "\n",
    "    raise NotImplementedError(f'Activation \"{act}\" not supported.')\n",
    "\n",
    "def layer(input_dim, output_dim, bias=True, act='relu', batch_norm=False, dropout=0.):\n",
    "    yield nn.Linear(input_dim, output_dim, bias=bias)\n",
    "\n",
    "    if batch_norm:\n",
    "        yield nn.BatchNorm1d(output_dim)\n",
    "    \n",
    "    if act is not None:\n",
    "        yield activation(act)\n",
    "\n",
    "    if dropout > 0.:\n",
    "        yield nn.Dropout(dropout)\n",
    "\n",
    "def mlp(layers, bias=True, act='relu', final_act=None, batch_norm=False, final_norm=False, dropout=0., final_drop=0.):\n",
    "    n_layers = len(layers)\n",
    "\n",
    "    for i in range(1, n_layers):\n",
    "        if i < n_layers - 1:\n",
    "            yield from layer(layers[i - 1], layers[i], bias, act, batch_norm, dropout)\n",
    "        else:\n",
    "            yield from layer(layers[i - 1], layers[i], bias, final_act, final_norm, final_drop)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers, bias=True, act='relu', final_act=None, batch_norm=False, final_norm=False, dropout=0., final_drop=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(*list(mlp(layers, bias, act, final_act, batch_norm, dropout, final_drop)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers, bias=True, act='relu', final_act='relu', batch_norm=False, final_norm=False, dropout=0., final_drop=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.e_net = MLP(layers[:-1], bias, act, final_act, batch_norm, final_norm, dropout, final_drop)\n",
    "        self.m_net = MLP(layers[-2:], bias, act, None, batch_norm, dropout=dropout)\n",
    "        self.v_net = MLP(layers[-2:], bias, act, None, batch_norm, dropou=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.e_net(x)\n",
    "        m = self.m_net(e)\n",
    "        v = self.v_net(e).exp()\n",
    "\n",
    "        return m, v\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers, bias=True, act='relu', final_act=None, batch_norm=False, final_norm=False, dropout=0., final_drop=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_net = MLP(layers, bias, act, final_act, batch_norm, final_norm, dropout, final_drop)\n",
    "\n",
    "    def forward(self, z):\n",
    "        d = self.d_net(z)\n",
    "\n",
    "        return d\n",
    "    \n",
    "class NTM(nn.Module, BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_topics=2, doc_size=50, vocab_size=50, layers=(25,), bias=True, act='relu', encoder_act='relu', decoder_act=None, batch_norm=True, final_norm=False, dropout=.2, final_drop=0., divergence=.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_topics = n_topics\n",
    "        self.doc_size = doc_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.layers = layers\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.encoder_act = encoder_act\n",
    "        self.decoder_act = decoder_act\n",
    "        self.batch_norm = batch_norm\n",
    "        self.final_norm = final_norm\n",
    "        self.dropout = dropout\n",
    "        self.final_drop = final_drop\n",
    "        self.divergence = divergence\n",
    "\n",
    "        self.encoder = Encoder(((vocab_size,) + layers + (n_topics,)), bias, act, encoder_act, batch_norm, batch_norm, dropout, final_drop)\n",
    "        self.decoder = Decoder((n_topics, vocab_size), bias, act, decoder_act, batch_norm, final_norm, dropout, final_drop)\n",
    "        self.optimizer = None\n",
    "        self.loss_log = []\n",
    "\n",
    "    def build(self, X, learning_rate=1e-2, batch_size=None):\n",
    "        self.optimizer = Adam(self.parameters(), learning_rate)\n",
    "        data_loader = DataLoader(X, X.shape[0] if batch_size is None else batch_size)\n",
    "        \n",
    "\n",
    "class VAE(nn.Module, BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, layers=(100, 10), bias=True, act='relu', encoder_act='relu', decoder_act=None, batch_norm=True, final_norm=False, dropout=.2, final_drop=0., divergence=.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.bias = bias\n",
    "        self.act = act\n",
    "        self.encoder_act = encoder_act\n",
    "        self.decoder_act = decoder_act\n",
    "        self.batch_norm = batch_norm\n",
    "        self.final_norm = final_norm\n",
    "        self.dropout = dropout\n",
    "        self.final_drop = final_drop\n",
    "        self.divergence = divergence\n",
    "\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.train_log = []\n",
    "        self.test_log = []\n",
    "\n",
    "    def build(self, X, learning_rate=1e-2, batch_size=None, test_size=.2):\n",
    "        if self.layers[0] != X.shape[-1]:\n",
    "            self.layers = (X.shape[-1], *self.layers)\n",
    "\n",
    "        self.encoder = Encoder(self.layers, self.bias, self.act, self.encoder_act, self.batch_norm, self.batch_norm, self.dropout, self.final_drop)\n",
    "        self.decoder = Decoder(self.layers[::-1], self.bias, self.act, self.decoder_act, self.batch_norm, self.final_norm, self.dropout, self.final_drop)\n",
    "        self.optimizer = Adam(self.parameters(), learning_rate)\n",
    "\n",
    "        if batch_size is None:\n",
    "            batch_size = int(X.shape[0]*(1. - test_size))//16\n",
    "\n",
    "        if test_size > 0.:\n",
    "            X_train, X_test = train_test_split(X, test_size=test_size)\n",
    "            train_loader, test_loader = DataLoader(X_train, batch_size), DataLoader(X_test, batch_size)\n",
    "        else:\n",
    "            train_loader, test_loader = DataLoader(X, batch_size), None\n",
    "\n",
    "        return train_loader, test_loader\n",
    "\n",
    "    def forward(self, x, return_divergence=False):\n",
    "        m, v = self.encoder(x)\n",
    "        z = m + v*torch.randn_like(v)\n",
    "\n",
    "        if return_divergence:\n",
    "            divergence = self.divergence*(m**2 + v**2 - v.log() - .5).sum()\n",
    "\n",
    "            return z, divergence\n",
    "        return z\n",
    "\n",
    "    def backward(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def step(self, loader, grad=True):\n",
    "        step_loss = 0.\n",
    "\n",
    "        for x in loader:\n",
    "            z, divergence = self(x, return_divergence=True)\n",
    "            y = self.decoder(z)\n",
    "            loss = (y - x).square().sum() + divergence\n",
    "            step_loss += self.backward(loss) if grad else loss.item()\n",
    "\n",
    "        step_loss /= loader.dataset.shape[0]\n",
    "\n",
    "        return step_loss\n",
    "\n",
    "    def fit(self, X, n_steps=150, learning_rate=1e-2, batch_size=None, test_size=.2, test_rate=1, desc='VAE', verbosity=1):\n",
    "        train_loader, test_loader = self.build(X, learning_rate, batch_size, test_size)\n",
    "\n",
    "        for i in tqdm(range(n_steps), desc) if verbosity == 1 else range(n_steps):\n",
    "            self.train_log.append(self.step(train_loader, grad=True))\n",
    "\n",
    "            if test_loader is not None and i%test_rate == 0:\n",
    "                self.test_log.append(self.step(test_loader, grad=False))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Z = self(X)\n",
    "\n",
    "        return Z.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
